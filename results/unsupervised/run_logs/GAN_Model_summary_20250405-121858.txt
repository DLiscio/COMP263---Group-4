==================================================
GAN FINAL MODEL TRAINING CONFIGURATION
==================================================

-------------------- Training Parameters --------------------
Batch Size: 64
Latent Dimension: 128
Epochs: 50

-------------------- Loss & Optimizers --------------------
Loss Function: BinaryCrossentropy

Generator Optimizer (Adam):
Learning Rate: {'module': 'keras.optimizers.schedules', 'class_name': 'ExponentialDecay', 'config': {'initial_learning_rate': 0.0002, 'decay_steps': 300, 'decay_rate': 0.95, 'staircase': False, 'name': 'ExponentialDecay'}, 'registered_name': None}
Beta 1: 0.5

Discriminator Optimizer (Adam):
Learning Rate: {'module': 'keras.optimizers.schedules', 'class_name': 'ExponentialDecay', 'config': {'initial_learning_rate': 0.0002, 'decay_steps': 300, 'decay_rate': 0.95, 'staircase': False, 'name': 'ExponentialDecay'}, 'registered_name': None}
Beta 1: 0.5
==================================================
MODEL ARCHITECTURES
==================================================

-------------------- Generator Architecture --------------------
Model: "sequential"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ dense (Dense)                        │ (None, 147456)              │      18,874,368 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ batch_normalization                  │ (None, 147456)              │         589,824 │
│ (BatchNormalization)                 │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ leaky_re_lu (LeakyReLU)              │ (None, 147456)              │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ reshape (Reshape)                    │ (None, 24, 24, 256)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_transpose (Conv2DTranspose)   │ (None, 48, 48, 128)         │         819,200 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ batch_normalization_1                │ (None, 48, 48, 128)         │             512 │
│ (BatchNormalization)                 │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ leaky_re_lu_1 (LeakyReLU)            │ (None, 48, 48, 128)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_transpose_1 (Conv2DTranspose) │ (None, 96, 96, 64)          │         204,800 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ batch_normalization_2                │ (None, 96, 96, 64)          │             256 │
│ (BatchNormalization)                 │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ leaky_re_lu_2 (LeakyReLU)            │ (None, 96, 96, 64)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_transpose_2 (Conv2DTranspose) │ (None, 96, 96, 32)          │          18,432 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ batch_normalization_3                │ (None, 96, 96, 32)          │             128 │
│ (BatchNormalization)                 │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ leaky_re_lu_3 (LeakyReLU)            │ (None, 96, 96, 32)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_transpose_3 (Conv2DTranspose) │ (None, 96, 96, 1)           │             289 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 20,507,809 (78.23 MB)
 Trainable params: 20,212,449 (77.10 MB)
 Non-trainable params: 295,360 (1.13 MB)


-------------------- Discriminator Architecture --------------------
Model: "sequential_1"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ conv2d (Conv2D)                      │ (None, 48, 48, 64)          │           1,664 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ leaky_re_lu_4 (LeakyReLU)            │ (None, 48, 48, 64)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ spatial_dropout2d (SpatialDropout2D) │ (None, 48, 48, 64)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_1 (Conv2D)                    │ (None, 24, 24, 128)         │         204,928 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ leaky_re_lu_5 (LeakyReLU)            │ (None, 24, 24, 128)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ spatial_dropout2d_1                  │ (None, 24, 24, 128)         │               0 │
│ (SpatialDropout2D)                   │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_2 (Conv2D)                    │ (None, 12, 12, 128)         │         147,584 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ leaky_re_lu_6 (LeakyReLU)            │ (None, 12, 12, 128)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten (Flatten)                    │ (None, 18432)               │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_1 (Dense)                      │ (None, 1)                   │          18,433 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 372,609 (1.42 MB)
 Trainable params: 372,609 (1.42 MB)
 Non-trainable params: 0 (0.00 B)


==================================================
TRAINING RESULTS FOLLOW BELOW
==================================================

Epoch, Time(s), Generator_Loss, Discriminator_Loss
1, 806.00, 1.7033, 0.9751
2, 871.16, 1.6073, 0.8857
3, 881.03, 1.1890, 1.1522
4, 885.95, 0.8618, 1.3122
5, 872.87, 0.8320, 1.3248
6, 872.98, 0.8152, 1.3455
7, 876.43, 0.7996, 1.3593
8, 870.71, 0.7725, 1.3818
9, 759.67, 0.7759, 1.3813
10, 709.27, 0.7693, 1.3777
11, 719.75, 0.7710, 1.3777
12, 715.28, 0.7744, 1.3734
13, 741.50, 0.7749, 1.3760
14, 738.97, 0.7737, 1.3760
15, 582.84, 0.7772, 1.3741
16, 491.39, 0.7827, 1.3667
17, 491.80, 0.7813, 1.3685
18, 492.43, 0.7818, 1.3660
19, 492.86, 0.7814, 1.3669
20, 493.37, 0.7802, 1.3678
21, 495.75, 0.7804, 1.3678
22, 491.68, 0.7787, 1.3686
23, 492.06, 0.7794, 1.3685
24, 491.60, 0.7777, 1.3690
25, 491.81, 0.7783, 1.3694
26, 493.99, 0.7884, 1.3614
27, 522.61, 0.8120, 1.3352
28, 513.96, 0.8074, 1.3370
29, 505.05, 0.8002, 1.3462
30, 753.25, 0.7921, 1.3534
31, 873.71, 0.7846, 1.3633
32, 878.03, 0.7826, 1.3689
33, 874.26, 0.7752, 1.3698
34, 880.98, 0.7741, 1.3813
35, 878.08, 0.7679, 1.3791
36, 880.66, 0.7698, 1.3791
37, 873.23, 0.7717, 1.3862
38, 877.69, 0.7658, 1.3794
39, 879.44, 0.7712, 1.3895
40, 878.28, 0.7666, 1.3800
41, 875.75, 0.7650, 1.3800
42, 873.21, 0.7663, 1.3799
43, 716.04, 0.7667, 1.3795
44, 695.45, 0.7658, 1.3794
45, 695.63, 0.7678, 1.3794
46, 730.85, 0.7659, 1.3790
47, 733.62, 0.7680, 1.3782
48, 737.27, 0.7671, 1.3788
49, 733.34, 0.7693, 1.3782
50, 734.00, 0.7668, 1.3777
