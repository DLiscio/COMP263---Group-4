==================================================
GAN FINAL MODEL TRAINING CONFIGURATION
==================================================

-------------------- Training Parameters --------------------
Batch Size: 32
Latent Dimension: 100
Epochs: 30

-------------------- Loss & Optimizers --------------------
Loss Function: BinaryCrossentropy

Generator Optimizer (Adam):
Learning Rate: {'module': 'keras.optimizers.schedules', 'class_name': 'ExponentialDecay', 'config': {'initial_learning_rate': 0.0001, 'decay_steps': 300, 'decay_rate': 0.99, 'staircase': False, 'name': 'ExponentialDecay'}, 'registered_name': None}
Beta 1: 0.5

Discriminator Optimizer (Adam):
Learning Rate: {'module': 'keras.optimizers.schedules', 'class_name': 'ExponentialDecay', 'config': {'initial_learning_rate': 0.0001, 'decay_steps': 300, 'decay_rate': 0.99, 'staircase': False, 'name': 'ExponentialDecay'}, 'registered_name': None}
Beta 1: 0.5
==================================================
MODEL ARCHITECTURES
==================================================

-------------------- Generator Architecture --------------------
Model: "sequential"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ dense (Dense)                        │ (None, 18432)               │       1,843,200 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ batch_normalization                  │ (None, 18432)               │          73,728 │
│ (BatchNormalization)                 │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ leaky_re_lu (LeakyReLU)              │ (None, 18432)               │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ reshape (Reshape)                    │ (None, 12, 12, 128)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_transpose (Conv2DTranspose)   │ (None, 24, 24, 64)          │         131,072 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ batch_normalization_1                │ (None, 24, 24, 64)          │             256 │
│ (BatchNormalization)                 │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ leaky_re_lu_1 (LeakyReLU)            │ (None, 24, 24, 64)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_transpose_1 (Conv2DTranspose) │ (None, 48, 48, 32)          │          32,768 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ batch_normalization_2                │ (None, 48, 48, 32)          │             128 │
│ (BatchNormalization)                 │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ leaky_re_lu_2 (LeakyReLU)            │ (None, 48, 48, 32)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_transpose_2 (Conv2DTranspose) │ (None, 96, 96, 1)           │             513 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 2,081,665 (7.94 MB)
 Trainable params: 2,044,609 (7.80 MB)
 Non-trainable params: 37,056 (144.75 KB)


-------------------- Discriminator Architecture --------------------
Model: "sequential_1"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ conv2d (Conv2D)                      │ (None, 48, 48, 32)          │             544 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ leaky_re_lu_3 (LeakyReLU)            │ (None, 48, 48, 32)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_1 (Conv2D)                    │ (None, 24, 24, 64)          │          32,832 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ leaky_re_lu_4 (LeakyReLU)            │ (None, 24, 24, 64)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_2 (Conv2D)                    │ (None, 12, 12, 128)         │         131,200 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ leaky_re_lu_5 (LeakyReLU)            │ (None, 12, 12, 128)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten (Flatten)                    │ (None, 18432)               │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_1 (Dense)                      │ (None, 1)                   │          18,433 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 183,009 (714.88 KB)
 Trainable params: 183,009 (714.88 KB)
 Non-trainable params: 0 (0.00 B)


==================================================
TRAINING RESULTS FOLLOW BELOW
==================================================

Epoch, Time(s), Generator_Loss, Discriminator_Loss
1, 89.57, 1.7642, 0.9355
2, 47.37, 0.9570, 1.2815
3, 46.04, 0.8300, 1.3778
4, 46.14, 0.8330, 1.3689
5, 45.76, 0.8130, 1.3837
6, 50.56, 0.8258, 1.3649
7, 47.93, 0.8463, 1.3323
8, 45.66, 0.8691, 1.3095
9, 72.11, 0.8786, 1.2984
10, 100.44, 0.9300, 1.2528
11, 58.65, 0.9856, 1.2167
12, 48.42, 1.0020, 1.2156
13, 46.58, 0.9877, 1.2338
14, 46.67, 1.0182, 1.2119
15, 46.11, 1.0111, 1.2195
16, 46.38, 1.0066, 1.2254
17, 46.86, 0.9892, 1.2420
18, 46.30, 0.9789, 1.2471
19, 47.20, 0.9731, 1.2527
20, 45.60, 0.9788, 1.2454
21, 55.04, 0.9701, 1.2539
22, 98.45, 0.9665, 1.2526
23, 100.30, 0.9689, 1.2550
24, 101.00, 0.9675, 1.2546
25, 99.90, 0.9628, 1.2546
26, 81.54, 0.9644, 1.2503
27, 99.38, 0.9726, 1.2456
28, 99.24, 0.9791, 1.2389
29, 99.23, 0.9787, 1.2384
30, 95.56, 0.9743, 1.2430
